<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Bellefair&family=DM+Sans:ital,opsz,wght@0,9..40,300;0,9..40,500;0,9..40,600;1,9..40,300;1,9..40,600&family=Goudy+Bookletter+1911&family=IM+Fell+Double+Pica&family=IM+Fell+English&family=Raleway:ital,wght@0,100;0,500;1,100&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link
      href="https://unpkg.com/gridjs/dist/theme/mermaid.min.css"
      rel="stylesheet"
    />
  </head>
  <body>
    <br><br>
    <h2>Question Answering Visualizations</h2>
    <h4>Paper: <a href="https://arxiv.org/abs/2212.09746">Evaluating Human-Language Model Interaction</a> | <a href="https://github.com/stanford-crfm/halie">Raw Data & Code</a> | Contact:  <a href="mailto:megha@cs.stanford.edu">megha@cs.stanford.edu</a></h4>
    <br>
    <p>We provide static visualizations of users querying 4 different language models (LMs) to answer questions from the MMLU dataset spanning 5 different categories: College Chemistry, Nutrition, Global Facts, Miscellaneous, and Foreign Policy. 
        You can use the search bar to search for a particular username, sort by language model, and click on the  <i>Open</i> button to open in your browser the visualization for any interaction trace.  
        <br><br>You can also filter by the final question answering accuracy for each interaction trace, to see how users who achieved higher accuracy with a given LM might have interacted differently!
      
      </p>
    <br><br>
    <div id="wrapper"></div>
    <br>
    <p>
    We additionally aggregate all interactions across MMLU question category splits, to make it easier to look at all user interactions for a given category and model and identify common failure modes: </p>
    <br>
    <div id="wrapper2"></div>
    <script src="https://unpkg.com/gridjs/dist/gridjs.umd.js"></script>
    <script src="src/index.js"></script>
  </body>
</html>
