<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Bellefair&family=DM+Sans:ital,opsz,wght@0,9..40,300;0,9..40,500;0,9..40,600;1,9..40,300;1,9..40,600&family=Goudy+Bookletter+1911&family=IM+Fell+Double+Pica&family=IM+Fell+English&family=Raleway:ital,wght@0,100;0,500;1,100&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link
      href="https://unpkg.com/gridjs/dist/theme/mermaid.min.css"
      rel="stylesheet"
    />
  </head>
  <body>
    <br><br>
    <h2>Question Answering Visualizations</h2>
    <h4>Paper: <a href="https://arxiv.org/abs/2212.09746">Evaluating Human-Language Model Interaction</a> | <a href="https://github.com/stanford-crfm/halie">Raw Data & Code</a> | Contact:  <a href="mailto:megha@cs.stanford.edu">megha@cs.stanford.edu</a></h4>
    <br>
    <p>We provide static visualizations of users querying 4 different language models (LMs) to answer questions from the MMLU dataset spanning 5 different categories: College Chemistry, Nutrition, Global Facts, Miscellaneous, and Foreign Policy. We also report the final question answering accuracy for each interaction trace.
        You can use the search bar above to search for a particular username, sort by language model, and click on the  <i>Open</i> button to open in your browser the visualization for a particular interaction trace.  
    </p>
    <br><br>
    <div id="wrapper"></div>
    <script src="https://unpkg.com/gridjs/dist/gridjs.umd.js"></script>
    <script src="src/index.js"></script>
  </body>
</html>
